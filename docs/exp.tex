\pgfplotstableread{data/tableinsert.dat}\mytableinsert
\pgfplotstableread{data/tablefind.dat}\mytablefind
\pgfplotstableread{data/pool.dat}\mytablepool

\begin{figure*}[ht]
  \centering
  \begin{adjustbox}{width=.8\textwidth}
    \subfigure[Latency per successful \texttt{insert}.]{
      \centering
      \begin{tikzpicture}[scale=.8]
        \begin{axis}[xlabel=N Threads, ylabel=Latency (usec), ymin=0, scaled ticks=false, tick label style={/pgf/number format/fixed},
            boxplot/draw direction=y,
            ytick distance={0.1},
            ymax={0.6},
            cycle list={{red},{blue},{black}},
            legend pos={north west}
          ]
          % hack to make legend
          \addlegendimage{area legend,fill=red,draw=black}; \addlegendentry{arr};
          \addlegendimage{area legend,fill=blue,draw=black}; \addlegendentry{ch};
          \addlegendimage{area legend,fill=black,draw=black}; \addlegendentry{tbb};

          \pgfplotsinvokeforeach{0,...,23}{
            \addplot+[boxplot prepared from table={table=\mytableinsert,
              row=#1, lower whisker=lw, upper whisker=uw, lower quartile=lq, upper quartile=uq,
            median=med, draw position=nthreads}, boxplot prepared] coordinates {};
          }
        \end{axis}
      \end{tikzpicture}
    }

    \subfigure[Latency per fail \texttt{insert} followed by an \texttt{erase}.]{
      \centering
      \begin{tikzpicture}[scale=.8]
        \begin{axis}[xlabel=N Threads, ylabel=Latency (usec), ymin=0, scaled ticks=false, tick label style={/pgf/number format/fixed},
            boxplot/draw direction=y,
            ytick distance={0.1},
            ymax={0.6},
            cycle list={{red},{blue},{black}},
            legend pos={north west}
          ]
          % hack to make legend
          \addlegendimage{area legend,fill=red,draw=black}; \addlegendentry{arr};
          \addlegendimage{area legend,fill=blue,draw=black}; \addlegendentry{ch};
          \addlegendimage{area legend,fill=black,draw=black}; \addlegendentry{tbb};

          \pgfplotsinvokeforeach{0,...,23}{
            \addplot+[boxplot prepared from table={table=\mytablefind,
              row=#1, lower whisker=lw, upper whisker=uw, lower quartile=lq, upper quartile=uq,
            median=med, draw position=nthreads}, boxplot prepared] coordinates {};
          }
        \end{axis}
      \end{tikzpicture}
    }
  \end{adjustbox}

  \caption{Latency of our hash-table implementation (\textit{arr}) in comparison
  to libcuckoo (\textit{ch}) and tbb concurrent hash map (\textit{tbb}).  Each
  hash-table is created with the initial size of $2^{16}$, the number of
  insertion per thread is chosen so that there is enough room and no expansion is
  required. TBB is also compiled with \textit{tbb-malloc} to improve performance.
  Latency exceeds an 0.5 microsecond is not shown.\label{fig:hash-table}}

\end{figure*}

\subsection{Component overheads}
In this section we evaluate our implementation of each individual component.
Understanding them individually gives us an idea on the minimum cost of the overall
system. 
\subsubsection{Concurrent Hash-Table}
To evaluate the overhead due to hash-table operations, we measure the latency 
on hash-table operations in the two following scenarios when performing them
in a number of POSIX threads:

\begin{itemize}
  \item A thread performs \texttt{insert} when there is no item with the same key.
  \item A thread performs \texttt{insert} and there is already items in the hash-table, and it subsequently
    performs \texttt{erase}.
\end{itemize}

 The two scenarios represent the only two possible runtime execution of the
 hash-table in our algorithms, thus measuring the latency in both cases give us
 an idea on how it will add to the overhead of MPI procedure. To justify the
 benefit of customization, we also compare ourselves to two popular general
 purposes hash-tables: libcuckoo implementing cuckoo hashing (\textit{ch})
 \cite{chasing}, and TBB concurrent hashmap (\textit{tbb}) \cite{tbb}. The
 experiment is to create threads and have each performed a fixed number of
 insertion (and erase) with different key to collect the mean latency. We ran
 the experiment for $1000$ times and collect the summaries of the latency of a
 thread. Between each run, we also perform a cache invalidation.

Figure~\ref{fig:hash-table} shows the result of our experiment. In both cases,
both TBB concurrent hash map and libcuckoo shows inconsistent latency when
there is more concurrent threads, which is the result of conflicts. Since our
hash-table is optimized for these patterns, our latency is almost always as low
as 50 nanosecond. This latency is consitent with our expectation that our cost
for the hash-table is equivalent to 2 memory writes.

\subsubsection{Concurrent Packet Pool}
\begin{figure}[ht]
%  \centering
%  \begin{adjustbox}{width=.8\textwidth}
%    \subfigure[Latency per successful \texttt{insert}.]{
      \centering
      \begin{tikzpicture}[scale=.8]
        \begin{axis}[xlabel=N Threads, ylabel=Latency (usec), ymin=0, scaled ticks=false, tick label style={/pgf/number format/fixed},
            boxplot/draw direction=y,
            %ytick distance={0.1},
            ymax={1},
            %cycle list={{red,dashed},{blue,dashed},{black,dashed},{red},{blue},{black}},
            cycle list={{red},{blue},{black}},
            legend pos={north west}
          ]
          % hack to make legend
          %\addlegendimage{area legend,fill=red,draw=black,dashed}; \addlegendentry{numa-cp};
          %\addlegendimage{area legend,fill=blue,draw=black,dashed}; \addlegendentry{stack-cp};
          %\addlegendimage{area legend,fill=black,draw=black,dashed}; \addlegendentry{queue-cp};
          \addlegendimage{area legend,fill=red,draw=black}; \addlegendentry{numa};
          \addlegendimage{area legend,fill=blue,draw=black}; \addlegendentry{stack};
          \addlegendimage{area legend,fill=black,draw=black}; \addlegendentry{queue};

          \pgfplotsinvokeforeach{0,...,23}{
            \addplot+[boxplot prepared from table={table=\mytablepool,
              row=#1, lower whisker=lw, upper whisker=uw, lower quartile=lq, upper quartile=uq,
            median=med, draw position=nthreads}, boxplot prepared] coordinates {};
          }
        \end{axis}
      \end{tikzpicture}
      \caption{Latency of pool implementation vs. a lockfree pool and a lockfree queue
        implementation, latency higher than 1 microsecond are not shown\label{fig:pool}.}
%    }
\end{figure}

The overhead due to packet pool is measured as the sum of the latency of
\texttt{get} and \texttt{ret} operation. We evaluate this quantity by
performing a random number of \texttt{get} followed by the same number of
\texttt{ret} in each thread. To match better with the real workload, we also
perform a random sleep in between the two group of operations. Further, The
range of the random number is chosen with the same seed for each implementation
and ensure threads do not request more than available packets: the maximum is
number of packets divided by number of threads. We perform this experiment
$1000$ times and plot the summary similarly as  the hash-table experiment.

The result is shown in Figure \ref{fig:pool} in comparison with implementation
using a concurrent lockfree stack and a lockfree queue. Our result for this
benchmark outperforms others by a wide margin, especially when the number of
threads increase. The lockfree stack is faster than the queue at single thread,
however performs worst for more than two threads since there is contention at
the top of the stack. The great variation in latency per operation of a
centralized pool is clearly due to memory conflicts in this type of access
patterns. Our latency is consistently in the range of 100-150 nanosecond.

\subsubsection{Thread scheduler}
The overhead due to thread scheduler is measured as scheduling time which consists
of two components:
\begin{itemize}
    \item Mark a thread as schedulable: the latency of a \texttt{ThreadSignal}.
    \item Finding schedulable threads: the latency taken when the scheduler is free
      until it finds a thread to execute i.e. time taken when thread is yielded until
      it is picked up again.
\end{itemize}

\subsection{Microbenchmarks and Application Evaluation}
\begin{figure*}[ht]
  \centering
  \begin{adjustbox}{width=.9\textwidth}
    \subfigure[Number of worker = 14, Number of threads = 14] {
      \begin{tikzpicture}[scale=.9]
        \begin{axis}[xlabel=Message Size (byte), ylabel=Latency (usec),
            xmode=log, ymode=log, xtick distance=2^2,
            log basis x=2, log basis y=2,
            ymin=1, ytick distance=2^2,
            xmax=2^14,
            xmin=1, % cycle list={{red},{blue},{black}},
            legend pos={north west},
            mark size=1.5pt,
            legend style={font=\tiny},
          ]
          \addplot table [x=threads, y=mvapich2+mt] {data/pingpong2.dat} node[pos=0.27, pin=below:127.6]{};
          \addlegendentry{mvapich2+mt}
          \addplot table [x=threads, y=pthread+hash] {data/pingpong2.dat} node [pos=0.25, pin=above:5.7]{};
          \addlegendentry{pthread+hash}
          \addplot table [x=threads, y=abt+hash] {data/pingpong2.dat} node [pos=0.25, pin=above:2.3]{};
          \addlegendentry{abt+hash}
          \addplot table [x=threads, y=fult+hash] {data/pingpong2.dat} node [pos=0.25, pin=above:1.7]{};
          \addlegendentry{fult+hash}
        \end{axis}
      \end{tikzpicture}
    }

    \subfigure[Single threaded latency test] {
      \centering
      \begin{tikzpicture}[scale=0.9]
        \begin{axis}[xlabel=Message Size (byte), ylabel=Latency (usec),
            xmode=log, ymode=log, xtick distance=2^2,
            log basis x=2, log basis y=2,
            xmax=2^14,
            xmin=1, % cycle list={{red},{blue},{black}},
            legend pos={north west},
            mark size=1.5pt,
            legend style={font=\tiny},
          ]
          \addplot table [x=threads, y=mvapich2+mt] {data/pingpong1.dat};
          \addlegendentry{mvapich2+mt}
          \addplot table [x=threads, y=pthread+hash] {data/pingpong1.dat};
          \addlegendentry{pthread+hash}
          \addplot table [x=threads, y=abt+hash] {data/pingpong1.dat};
          \addlegendentry{abt+hash}
          \addplot table [x=threads, y=fult+hash] {data/pingpong1.dat};
          \addlegendentry{fult+hash}
          \addplot table [x=threads, y=mvapich2] {data/pingpong1.dat};
          \addlegendentry{mvapich2}
        \end{axis}
      \end{tikzpicture}
    }

%    \subfigure[Number of worker = 14, Number of threads = 42] {
%      \begin{tikzpicture}[scale=.9]
%        \begin{axis}[xlabel=Message Size (byte), ylabel=Latency (usec),
%            xmode=log, ymode=log, xtick distance=2^2,
%            log basis x=2, log basis y=2,
%            ymin=1, ytick distance=2^2,
%            xmax=2^14,
%            xmin=1, % cycle list={{red},{blue},{black}},
%            legend pos={north west},
%            mark size=1.5pt,
%            legend style={font=\tiny},
%          ]
%          \addplot table [x=threads, y=mvapich2+mt] {data/pingpong3.dat};
%          \addlegendentry{mvapich2+mt}
%          \addplot table [x=threads, y=pthread+hash] {data/pingpong3.dat};
%          \addlegendentry{pthread+hash}
%          \addplot table [x=threads, y=abt+hash] {data/pingpong3.dat};
%          \addlegendentry{abt+hash}
%          \addplot table [x=threads, y=fult+hash] {data/pingpong3.dat};
%          \addlegendentry{fult+hash}
%        \end{axis}
%      \end{tikzpicture}
%    }
  \end{adjustbox}

  \caption{Latency comparison between different MPI implementation using OSU latency test.\label{fig:osumt}}
\end{figure*}

\subsubsection{OSU latency benchmarks}
\begin{table}
\begin{tabular}{|l|l|l|l|}
\hline
MPI impl. & Matching & Scheduler & Speedup \\
\hline
mvapich2+mt & queues & POSIX thread & 1 \\
\hline
pthread+hash & hash-table & POSIX thread & 22 \\
\hline
abt+hash & hash-table & Argobots & 56 \\
\hline
fult+hash & hash-table & Bitvector & 73 \\
\hline
\end{tabular}
\caption{Summary of MPI implementation used in the evaluation, Speedup is shown
  for latency test in Figure~\ref{fig:osumt} for message size of 64 bytes in
  comparison to mvapich2+mt.\label{tbl:mpi}}
\end{table}

We use OSU benchmarks \cite{osubench} to evaluate the latency per
\texttt{MPI_Send} or \texttt{MPI_Recv}. The single threaded test is performed
using \texttt{osu_latency}, the multi-threaded test is performed with
\texttt{osu_latency_mt}. We compare our best implementation (fult+hash) with
MVAPICH2 and other possible implementations of our runtime design.
Table~\ref{tbl:mpi} summerizes the differences of the tested runtime.
\textit{mvapich2+mt} is the MVAPICH2 with POSIX thread which is the base
implementation in \texttt{osu_latency_mt}. Others are our runtime
with different thread scheduler: POSIX thread, Argobots and the described
bit-vector based scheduler.

Performance results for multi-threaded tests are shown in
Figure~\ref{fig:osumt}.  Under tested settings, the most improvement in
performance is due to the replacement of the complex queue of MPI with the
hash-table.  Then, the replacement of POSIX threads with a light-weight
threads. Our thread-scheduler also improves the latency upto 40\% compared
to Argobots.
