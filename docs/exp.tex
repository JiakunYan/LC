\pgfplotstableread{data/tableinsert.dat}\mytableinsert
\pgfplotstableread{data/tablefind.dat}\mytablefind
\pgfplotstableread{data/pool.dat}\mytablepool

\begin{figure*}[ht]
  \centering
  \begin{adjustbox}{width=.8\textwidth}
    \subfigure[Latency per successful \texttt{insert}.]{
      \centering
      \begin{tikzpicture}[scale=.8]
        \begin{axis}[xlabel=N Threads, ylabel=Latency (usec), ymin=0, scaled ticks=false, tick label style={/pgf/number format/fixed},
            boxplot/draw direction=y,
            ytick distance={0.1},
            ymax={0.6},
            cycle list={{red},{blue},{black}},
            legend pos={north west}
          ]
          % hack to make legend
          \addlegendimage{area legend,fill=red,draw=black}; \addlegendentry{arr};
          \addlegendimage{area legend,fill=blue,draw=black}; \addlegendentry{ch};
          \addlegendimage{area legend,fill=black,draw=black}; \addlegendentry{tbb};

          \pgfplotsinvokeforeach{0,...,23}{
            \addplot+[boxplot prepared from table={table=\mytableinsert,
              row=#1, lower whisker=lw, upper whisker=uw, lower quartile=lq, upper quartile=uq,
            median=med, draw position=nthreads}, boxplot prepared] coordinates {};
          }
        \end{axis}
      \end{tikzpicture}
    }

    \subfigure[Latency per fail \texttt{insert} followed by an \texttt{erase}.]{
      \centering
      \begin{tikzpicture}[scale=.8]
        \begin{axis}[xlabel=N Threads, ylabel=Latency (usec), ymin=0, scaled ticks=false, tick label style={/pgf/number format/fixed},
            boxplot/draw direction=y,
            ytick distance={0.1},
            ymax={0.6},
            cycle list={{red},{blue},{black}},
            legend pos={north west}
          ]
          % hack to make legend
          \addlegendimage{area legend,fill=red,draw=black}; \addlegendentry{arr};
          \addlegendimage{area legend,fill=blue,draw=black}; \addlegendentry{ch};
          \addlegendimage{area legend,fill=black,draw=black}; \addlegendentry{tbb};

          \pgfplotsinvokeforeach{0,...,23}{
            \addplot+[boxplot prepared from table={table=\mytablefind,
              row=#1, lower whisker=lw, upper whisker=uw, lower quartile=lq, upper quartile=uq,
            median=med, draw position=nthreads}, boxplot prepared] coordinates {};
          }
        \end{axis}
      \end{tikzpicture}
    }
  \end{adjustbox}

  \caption{Latency of our hash-table implementation (\textit{arr}) in comparison
  to libcuckoo (\textit{ch}) and tbb concurrent hash map (\textit{tbb}).  Each
  hash-table is created with the initial size of $2^{16}$, the number of
  insertion per thread is chosen so that there is enough room and no expansion is
  required. TBB is also compiled with \textit{tbb-malloc} to improve performance.
  Latency exceeds an 0.5 microsecond is not shown.\label{fig:hash-table}}

\end{figure*}

\subsection{Component overheads}
In this section we evaluate our implementation of each individual component.
Understanding them individually gives us an idea on the minimum cost of the overall
system. 
\subsubsection{Concurrent Hash-Table}
To evaluate the overhead due to hash-table operations, we measure the latency 
on hash-table operations in the two following scenarios when performing them
in a number of POSIX threads:

\begin{itemize}
  \item A thread performs \texttt{insert} when there is no item with the same key.
  \item A thread performs \texttt{insert} and there is already items in the hash-table, and it subsequently
    performs \texttt{erase}.
\end{itemize}

 The two scenarios represent the only two possible runtime execution of the
 hash-table in our algorithms, thus measuring the latency in both cases give us
 an idea on how it will add to the overhead of MPI procedure. To justify the
 benefit of customization, we also compare ourselves to two popular general
 purposes hash-tables: libcuckoo implementing cuckoo hashing (\textit{ch})
 \cite{chasing}, and TBB concurrent hashmap (\textit{tbb}) \cite{tbb}. The
 experiment is to create threads and have each performed a fixed number of
 insertion (and erase) with different key to collect the mean latency. We ran
 the experiment for $1000$ times and collect the summaries of the latency of a
 thread. Between each run, we also perform a cache invalidation.

Figure~\ref{fig:hash-table} shows the result of our experiment. In both cases,
both TBB concurrent hash map and libcuckoo shows inconsistent latency when
there is more concurrent threads, which is the result of conflicts. Since our
hash-table is optimized for these patterns, our latency is almost always as low
as 50 nanosecond. This latency is consitent with our expectation that our cost
for the hash-table is equivalent to 2 memory writes.

\subsubsection{Thread scheduler}

\begin{table}[h]
  \begin{tabular}{| l || l | l | l |}
    \hline
    & \textbf{POSIX thread} & \textbf{Argobots}	& \textbf{Fult} \\ \hline
    Scheduling & 0.75 & 0.08 & 0.02 \\ \hline
    Signal & 1.15 & 0.30 & 0.01 \\ \hline
    Total & 1.90 & 0.38 & 0.03 \\ \hline
  \end{tabular}
  \caption{Break down of thread scheduler overheads, shown in $usec$. \label{fig:thread}}
\end{table}

To evaluate the thread scheduler minimum overhead, we measure separately the
two operations: 1) How fast can a thread be scheduled - by performing a large
number of yield within a thread and obtain the entire runtime. 2) The cost of a
\texttt{ThreadSignal} by getting the runtime latency when performing operation
on a thread of a different worker. Table~\ref{fig:thread} shows our results,
Fult scheduler achieves the overall cost of 30 nanosecond, about $10\times$
that of Argobots and $60\times$ of POSIX threads.

\subsubsection{Concurrent Packet Pool}
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[scale=.9]
    \begin{axis}[xlabel=N Threads, ylabel=Latency (usec), ymin=0, scaled ticks=false, tick label style={/pgf/number format/fixed},
        boxplot/draw direction=y,
            %ytick distance={0.1},
        ymax={1},
            %cycle list={{red,dashed},{blue,dashed},{black,dashed},{red},{blue},{black}},
        cycle list={{red},{blue},{black}},
        legend pos={north west}
      ]
          % hack to make legend
          %\addlegendimage{area legend,fill=red,draw=black,dashed}; \addlegendentry{numa-cp};
          %\addlegendimage{area legend,fill=blue,draw=black,dashed}; \addlegendentry{stack-cp};
          %\addlegendimage{area legend,fill=black,draw=black,dashed}; \addlegendentry{queue-cp};
      \addlegendimage{area legend,fill=red,draw=black}; \addlegendentry{numa};
      \addlegendimage{area legend,fill=blue,draw=black}; \addlegendentry{stack};
      \addlegendimage{area legend,fill=black,draw=black}; \addlegendentry{queue};

      \pgfplotsinvokeforeach{0,...,23}{
        \addplot+[boxplot prepared from table={table=\mytablepool,
          row=#1, lower whisker=lw, upper whisker=uw, lower quartile=lq, upper quartile=uq,
        median=med, draw position=nthreads}, boxplot prepared] coordinates {};
      }
    \end{axis}
  \end{tikzpicture}
  \caption{Latency of pool implementation vs. a lockfree pool and a lockfree queue
    implementation, latency higher than 1 microsecond are not shown\label{fig:pool}.}
  \end{figure}

The overhead due to packet pool is measured as the sum of the latency of
\texttt{get} and \texttt{ret} operation. We evaluate this quantity by
performing a random number of \texttt{get} followed by the same number of
\texttt{ret} in each thread. To match better with the real workload, we also
perform a random sleep in between the two group of operations. Further, The
range of the random number is chosen with the same seed for each implementation
and ensure threads do not request more than available packets: the maximum is
number of packets divided by number of threads. We perform this experiment
$1000$ times and plot the summary similarly as  the hash-table experiment.

The result is shown in Figure \ref{fig:pool} in comparison with implementation
using a concurrent lockfree stack and a lockfree queue. Our result for this
benchmark outperforms others by a wide margin, especially when the number of
threads increase. The lockfree stack is faster than the queue at single thread,
however performs worst for more than two threads since there is contention at
the top of the stack. The great variation in latency per operation of a
centralized pool is clearly due to memory conflicts in this type of access
patterns. Our latency is consistently in the range of 100-150 nanosecond.


\subsection{Microbenchmarks and Application Evaluation}
\begin{figure*}[ht]
  \centering
  \begin{adjustbox}{width=.9\textwidth}
    \subfigure[Latency per message transfer for $14$ threads, one per worker/socket.] {
      \centering
      \begin{tikzpicture}[scale=.9]
        \begin{axis}[xlabel=Message Size (byte), ylabel=Latency (usec),
            xmode=log, ymode=log, xtick distance=2^2,
            log basis x=2, log basis y=2,
            ymin=1, ytick distance=2^2,
            xmax=2^14,
            xmin=1, % cycle list={{red},{blue},{black}},
            legend pos={north west},
            mark size=1.5pt,
            legend style={font=\tiny},
            ylabel near ticks,
            xlabel near ticks,
          ]
          \addplot table [x=threads, y=mvapich2+mt] {data/pingpong2.dat} node[pos=0.27, pin=below:40.1]{};
          \addlegendentry{mvapich2+mt}
          \addplot table [x=threads, y=pthread+hash] {data/pingpong2.dat} node [pos=0.25, pin=above:5.5]{};
          \addlegendentry{pthread+hash}
          \addplot table [x=threads, y=abt+hash] {data/pingpong2.dat} node [pos=0.25, pin=above:2.3]{};
          \addlegendentry{abt+hash}
          \addplot table [x=threads, y=fult+hash] {data/pingpong2.dat} node [pos=0.25, pin=above:1.7]{};
          \addlegendentry{fult+hash}
        \end{axis}
      \end{tikzpicture}
    }
    \subfigure[Latency per 64-byte message transfer for upto 1M thread and round-robin assigned to worker/socket, 
      \textit{pthread+hash} version only works up to 16K threads due to limitation in the system.] {
      \centering
      \begin{tikzpicture}[scale=.9]
        \begin{axis}[xlabel=N threads, ylabel=Latency (usec),
            xmode=log, ymode=log, xtick distance=2^2,
            log basis x=2, log basis y=2,
            ymin=1, ytick distance=2^2,
            xmin=4,
            legend pos={north west},
            mark size=1.5pt,
            legend style={font=\tiny},
            yticklabel=\pgfmathparse{2^\tick}\pgfmathprintnumber{\pgfmathresult},
            ylabel near ticks,
            xlabel near ticks,
          ]
          \addplot table [x=threads, y=pthread+hash,col sep=comma] {data/strong.dat} node[pos=0.9, pin=7.9] {};
          \addlegendentry{pthread+hash}
          \addplot table [x=threads, y=abt+hash, col sep=comma] {data/strong.dat} node[pos=0.55, pin=2.3] {};
          \addlegendentry{abt+hash}
          \addplot table [x=threads, y=fult+hash, col sep=comma] {data/strong.dat} node[pos=0.55, pin=1.7] {};
          \addlegendentry{fult+hash}
        \end{axis}
      \end{tikzpicture}
    }
%    \subfigure[Number of worker = 14, Number of threads = 42] {
%      \begin{tikzpicture}[scale=.9]
%        \begin{axis}[xlabel=Message Size (byte), ylabel=Latency (usec),
%            xmode=log, ymode=log, xtick distance=2^2,
%            log basis x=2, log basis y=2,
%            ymin=1, ytick distance=2^2,
%            xmax=2^14,
%            xmin=1, % cycle list={{red},{blue},{black}},
%            legend pos={north west},
%            mark size=1.5pt,
%            legend style={font=\tiny},
%          ]
%          \addplot table [x=threads, y=mvapich2+mt] {data/pingpong3.dat};
%          \addlegendentry{mvapich2+mt}
%          \addplot table [x=threads, y=pthread+hash] {data/pingpong3.dat};
%          \addlegendentry{pthread+hash}
%          \addplot table [x=threads, y=abt+hash] {data/pingpong3.dat};
%          \addlegendentry{abt+hash}
%          \addplot table [x=threads, y=fult+hash] {data/pingpong3.dat};
%          \addlegendentry{fult+hash}
%        \end{axis}
%      \end{tikzpicture}
%    }
  \end{adjustbox}

  \caption{Latency comparison between different MPI implementation using OSU multi-threaded latency test.\label{fig:osumt}}
\end{figure*}

\subsubsection{OSU latency benchmarks}
\begin{table}[h]
\begin{tabular}{|l||l|l|}
\hline
& \textbf{Matching Algorithm} & \textbf{Scheduler} \\
\hline
mvapich2+mt & queues & POSIX thread \\
\hline
pthread+hash & hash-table & POSIX thread \\
\hline
abt+hash & hash-table & Argobots \\
\hline
fult+hash & hash-table & Fult \\
\hline
\end{tabular}
\caption{Summary of MPI implementation used in the evaluation, Speedup is shown
  for latency test in Figure~\ref{fig:osumt} for message size of 64 bytes in
  comparison to mvapich2+mt.\label{tbl:mpi}}
\end{table}

We use OSU benchmarks \cite{osubench} to evaluate the latency per
\texttt{MPI_Send} or \texttt{MPI_Recv}. The single threaded test is performed
using \texttt{osu_latency} and Infiniband latency testsuite, the multi-threaded
test is performed with \texttt{osu_latency_mt}. We compare our best
implementation (\textit{fult+hash}) with MVAPICH2 and other possible
implementation of our runtime design. Table~\ref{tbl:mpi} summarizes the
differences of the tested runtime.  \textit{mvapich2+mt} is the MVAPICH2 with
POSIX thread i.e. \textit{osu_latency_mt} benchmark. For a fair comaprison with
MVAPICH2, we also disable their \textit{RDMA fast path} and \textit{shared
memory} optimization, (by setting \texttt{MV2_USE_RDMA_FAST_PATH=0},
\texttt{MV2_USE_SHARED_MEM=0}). Further, in multi-threaded test, we modify the
code so that each thread uses different tags.

Performance results for multi-threaded tests are shown in
Figure~\ref{fig:osumt}.  Under tested settings, the most improvement in
performance is due to the replacement of the complex queue of MPI with the
hash-table.  Then, the replacement of POSIX threads with a light-weight
threads. Our thread-scheduler improves the latency upto 40\% compared to
Argobots, $3$ times compared to POSIX thread scheduler. Overally, we achieve
speedup of upto $25\times$ compared to MVAPICH2.

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=0.9]
    \begin{axis}[xlabel=Message Size (byte), ylabel=Latency (usec),
        xmode=log, ymode=log, xtick distance=2^2,
        log basis x=2, log basis y=2,
        xmax=2^14,
        xmin=1, % cycle list={{red},{blue},{black}},
        legend pos={north west},
        mark size=1.5pt,
        legend style={font=\tiny},
      ]
      \addplot table [x=threads, y=mvapich2+mt] {data/pingpong1.dat} node[pos=0.24, pin=above:1.55]{};
      \addlegendentry{mvapich2+mt}
      \addplot table [x=threads, y=pthread+hash] {data/pingpong1.dat};
      \addlegendentry{pthread+hash}
      \addplot table [x=threads, y=abt+hash] {data/pingpong1.dat} node[pos=0.24, pin=above:1.85]{};
      \addlegendentry{abt+hash}
      \addplot table [x=threads, y=fult+hash] {data/pingpong1.dat} node[pos=0.24, pin=right:1.45]{};
      \addlegendentry{fult+hash}
      \pgfplotsset{cycle list shift=1}
      \addplot table [x=threads, y=mvapich2] {data/pingpong1.dat};
      \addlegendentry{mvapich2}
      \addplot table [x=threads, y=ibv] {data/pingpong1.dat} node[pos=0.24, pin=right:1.23]{};
      \addlegendentry{device latency}
    \end{axis}
  \end{tikzpicture}
  \caption{Latency comparison for single-threaded OSU latency test.\label{fig:osu-single}}
\end{figure}

Our overheads analyzed in the previous section translate accurately to the
single-threaded benchmark results in Figure~\ref{fig:osu-single}. In general,
compared to a single-threaded MPI implementation, our best implementation
achieves overall overhead of 0.2 $usec$, lower than running MPI
in a POSIX thread in MPI_THREAD_MULTIPLE mode (as in \textit{mvapich2+mt}) and virtually
tie with a single thread performance (as in \textit{mvapich2}).
