Message Passing Interface (MPI) has been implemented by several vendors. On
Blue Gene machine, MPICH \cite{mpich} is the de-factor implementation, likewise
on Infiniband machine, we have MVAPICH \cite{mvapich}. Cray has their own
customization on top of MPICH called CrayMPI; Intel similarly implements
IntelMPI. OpenMPI is another effort from Open-source collaboration of several
industry and academic vendors \cite{openMPI}.  Each vendor relies on 
their best knowledge of the underlying computing machine and network
architecture to optimize their implementation. For examples, the implementation
might reduce latency by accessing directly the low-level network-API or
optimize critical path using specialized instructions.

Recently, because of the increasing in the number of processor cores within a
computing board, there raises a need of running MPI efficiently in threaded
environments. Although MPI specification requires all of its procedures to be
thread-safe, MPI implementations are not originally designed to optimize for
using multiple threads: OpenMPI latest version still does not
provide a stable support of MPI\_THREAD\_MUTLIPLE \cite{openmpifr},  MPICH suports thread-safety
via a coarse-grain locking which essentially serialized all MPI code. The
research to replace coarse-grain locking is still undergoning \cite{mpilock}, but the
progress is slow because the existing software stack is only optimized for
single-core performance. For example, there are significant number of global
variables shared among various component of a MPICH. Global variables which
implements stack, queue or hash-table can be replaced with concurrent data
structure, others might be protected with finer-grains lock.  However, locks
and concurrent data structure has overhead could be added to  the critical path
and reduces performance of single-threaded applications.

On the other hand, communication in multiple threads is troublesome for many
reasons. Data movement between processors could cache trashing or false sharing
if taken care of. Processors can becomes under-ultilize because threads execute
I/O operations can be preempted. Resource management have to consider locality
and efficient concurrent accesses since they are shared among processors. Last
but not least, kernel thread context-switching is costly, increasingly with the
number of threads. Several studies have reported significant overhead for
performing MPI concurrently in many threads \cite{threadissue, mpilock}.

Because of the lack of efficient multi-threaded implementation, to our best
knowledge, there is no production application that uses the thread-safety
feature of MPI. What has been ended up is application running MPI in a ``Thread
Funnelled'' mode i.e. only one thread is allowed to execute MPI procedure;
other threads are used only for computation. This leads to a fairly restrictive
programming model, in which communication and computation code appears in
different phases or procedures. This approach often uses MPI non-blocking
features to allow overlapping between computation and communication and hope
the MPI runtime will be able to make communication progress behind the scene.
In practice, to support better asynchronous communication, user of MPI
implementations often need to enable a flag to request the runtime to spawn a
dedicated polling thread for progressing the communication (e.g.,
MPICH_ASYNC_PROGRESS flag in MPICH/MVAPICH).

The solution that we have seen for existing MPI implementation to adopt
multi-core machine is thus ad-hoc and inefficient. In this paper, we present an
approximate implementation of MPI point-to-point communication which takes into
account of multi-core machine by design. Our implementation is approximate
since we shall relax some semantics requirement of MPI. We argue that our
design and algorithms for this relaxation is able to achieve efficient
implementation, yet do not restrict applications from using MPI effectively.
More complex MPI semantics and functionality can be built on top of our
point-to-point communication. By using this bottom-up strategy, we provide an
insight on the minimal cost to implement MPI, accurately to the number of
memory accesses. We summarize our most signfinicant contributions as follows:
\begin{itemize}
  \item A light-weight thread scheduler using bit-vector which achieves
single-instruction for signalling continuation. 
  \item An constant time overhead algorithm for MPI point-to-point communication
utilising a specially designed concurrent wait-free hash-table.
  \item A resource-aware locality-aware concurrent memory pool for packet management.
  \item A MPI runtime design for scaling up to million communicating threads.
\end{itemize}

Our paper is organized as follows. The next section describes our runtime
architecture from the high-level design to individual components. We present
our implementation details and optimization in Section \ref{sec:impl}. Section
\ref{sec:exp} discusses experiment and results. Section \ref{sec:related} gives
an overview of some related works. Finally Section \ref{sec:conclusion} concludes
our study.
